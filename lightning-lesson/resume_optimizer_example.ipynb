{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31843b4-80d3-4832-8e4a-5722842bee87",
   "metadata": {},
   "source": [
    "# Lightning Lesson: How to Build a Resume Optimizer with AI\n",
    "\n",
    "Code authored by: Shaw Talebi\n",
    "\n",
    "[Recording link](https://youtu.be/R5WXaxmb6m4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bfd72-71d5-425a-8b39-3cbcbbabc1f1",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7519d8c-5f98-43af-9898-2ea65b7fdc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from openai import OpenAI\n",
    "from top_secret import my_sk\n",
    "\n",
    "from markdown import markdown\n",
    "from weasyprint import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81122eb-7584-4ea7-826b-7b628998ac88",
   "metadata": {},
   "source": [
    "### 1) Import Resume & Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eadad78-80c6-451b-a0f6-86c62d3e2690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Shaw Talebi  \n",
       "**Email**: [shawhintalebi@gmail.com](mailto:shawhintalebi@gmail.com)  \n",
       "**Homepage**: [shawhintalebi.com](https://shawhintalebi.com/) | **LinkedIn**: [shawhintalebi](https://www.linkedin.com/in/shawhintalebi/)  \n",
       "\n",
       "---\n",
       "\n",
       "## Education  \n",
       "**The University of Texas at Dallas**  \n",
       "- PhD, Physics - *May 2022*  \n",
       "- M.S., Physics - *December 2019*  \n",
       "- B.S., Physics - *May 2017*  \n",
       "\n",
       "## Technical Skills  \n",
       "- **Tools**: Python, SQL, GitHub, AWS (SageMaker), Snowflake, Julia  \n",
       "- **Certifications**:  AWS Cloud Practitioner Essentials (AWS), Data Structure & Algorithms (Udemy), Tableau (Udemy)  \n",
       "\n",
       "## Work Experience\n",
       "\n",
       "### **Founder**  \n",
       "**The Data Entrepreneurs** | Plano, Texas *(January 2023 - Present)*  \n",
       "- Founded a community of 900+ data & AI entrepreneurs. Hosting monthly workshops and podcast episodes.\n",
       "\n",
       "### **Content Creator**  \n",
       "**YouTube** | Plano, Texas *(September 2020 - Present)*  \n",
       "- Making videos on Data Science + Entrepreneurship. 1M+ Views. 35k+ subs.\n",
       "\n",
       "### **Writer**  \n",
       "**Towards Data Science** | Plano, Texas *(November 2020 - Present)*  \n",
       "- Writing articles on Data Science + AI. 25k Monthly Reads. 16k Followers.\n",
       "\n",
       "### **Data Science Consultant**  \n",
       "**Shawhin Talebi Ventures LLC** | Plano, Texas *(December 2020 - Present)*  \n",
       "- Implemented full data pipeline for novel study evaluating the impact of over 300 biometrics variables on human performance in live-fire training scenarios which uncovered link between EEG activity and performance.  \n",
       "- Applied unsupervised learning approaches to longitudinal ICU data to discover sepsis sub-phenotypes.  \n",
       "\n",
       "### **Data Scientist**  \n",
       "**Toyota Financial Services** | Plano, Texas *(June 2022 - July 2023)*  \n",
       "- Uncovered and corrected issue in a pre-existing production credit risk model that impacted over 70% accounts and wrote model monitoring scripts to help avoid future failures.  \n",
       "- Redeveloped loan originations model for independent dealers which resulted in a 50% model performance improvement and provided $2.5 million in realized value to business partner.  \n",
       "\n",
       "### **Research Assistant**  \n",
       "**The University of Texas at Dallas (Department of Physics)** | Richardson, Texas *(December 2018 - May 2022)*  \n",
       "- Published open-source methodology to discover optimal EEG bands which led to better characterization of the underlying power spectrum than the more commonly used band boundaries by a factor of 2.  \n",
       "- Trained over 100 machine learning models to estimate particulate matter (PM) concentrations based on a suite of over 300 biometric variables, achieving high fidelity (r² = 0.91).  \n",
       "- Led 5 teams focused on deployment of a real-time Python-based biometrics application, unveiling immediate insights that were previously inaccessible.  \n",
       "\n",
       "## Awards and Honors  \n",
       "- **2021 Friends of BrainHealth Visionary New Scientist Award** — Finalist *(September 2021)*  \n",
       "- **2nd Annual Weeks of Welcome Poster Competition** — 3rd Place Winner *(August 2019)*  \n",
       "- **Outstanding Undergraduate Student** — Nominee *(April 2017)*  \n",
       "- **Student Leader of the Year** — Nominee *(April 2017)*\n",
       "\n",
       "---\n",
       "\n",
       "## Publications  \n",
       "\n",
       "1. Talebi S., Lary D.J., Wijeratne L. OH., and Lary, T. *Modeling Autonomic Pupillary Responses from External Stimuli Using Machine Learning* (2019). DOI: [10.26717/BJSTR.2019.20.003446](https://doi.org/10.26717/BJSTR.2019.20.003446).  \n",
       "\n",
       "2. Wijeratne, L.O.; Kiv, D.R.; Aker, A.R.; Talebi, S.; Lary, D.J. *Using Machine Learning for the Calibration of Airborne Particulate Sensors*. Sensors 2020, 20, 99.  \n",
       "\n",
       "3. Lary, D.J. et al. *Autonomous Learning of New Environments with a Robotic Team Employing Hyper-Spectral Remote Sensing, Comprehensive In-Situ Sensing and Machine Learning*. Sensors 2021, 21, 2240. [DOI](https://doi.org/10.3390/s21062240).  \n",
       "\n",
       "4. Zhang, Y.; Wijeratne, L.O.H.; Talebi, S.; Lary, D.J. *Machine Learning for Light Sensor Calibration*. Sensors 2021, 21, 6259. [DOI](https://doi.org/10.3390/s21186259).  \n",
       "\n",
       "5. Talebi, S. et al. *Data-Driven EEG Band Discovery with Decision Trees*. Sensors 2022, Vol. 22, Page 3048, 22(8), 3048. [DOI](https://doi.org/10.3390/S22083048).  \n",
       "\n",
       "6. Fernando, B.A. et al. *Unsupervised Blink Detection Using Eye Aspect Ratio Values*. Preprints 2022, [DOI](https://doi.org/10.20944/preprints202203.0200.v1).  \n",
       "\n",
       "7. Talebi, S. et al. *Decoding Physical and Cognitive Impacts of PM Concentrations at Ultra-fine Scales*. [Research Square DOI](https://doi.org/10.21203/rs.3.rs-1499191/v1).  \n",
       "\n",
       "8. Lary, D.J. et al. *Machine Learning, Big Data, and Spatial Tools: A Combination to Reveal Complex Facts That Impact Environmental Health*. Springer, Cham. [DOI](https://doi.org/10.1007/978-3-030-71377-5_12).  \n",
       "\n",
       "9. Wijerante, L.O.H. et al. *Advancement in Airborne Particulate Estimation Using Machine Learning*. Springer, Cham. [DOI](https://doi.org/10.1007/978-3-030-71377-5_13).  \n",
       "\n",
       "10. Waczak, J. et al. *Characterizing Water Composition with an Autonomous Robotic Team Employing Comprehensive In Situ Sensing, Hyperspectral Imaging, Machine Learning, and Conformal Prediction*. Remote Sens. 2024, 16, 996. [DOI](https://doi.org/10.3390/rs16060996).  \n",
       "\n",
       "11. Ruwali, S. et al. *Quantifying Inhaled Concentrations of Particulate Matter, Carbon Dioxide, Nitrogen Dioxide, and Nitric Oxide Using Observed Biometric Responses with Machine Learning*. Preprints 2024. [DOI](https://doi.org/10.20944/preprints202402.0697.v1).  \n",
       "\n",
       "---\n",
       "\n",
       "**References and additional information available upon request.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open and read the Markdown file\n",
    "with open(\"resumes/resume.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    resume_string = file.read()\n",
    "\n",
    "# display resume\n",
    "display(Markdown(resume_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11562042-ba1a-4703-b38e-03f96031bee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Job Description  Great Expectations' mission is to revolutionize the speed and integrity of data collaboration. Our SaaS data quality platform, GX Cloud, provides end-to-end support for building trust in your data. It's powered by GX Core, our open source Python library and one of the most widely-used data quality solutions in the world. GX Core leverages extensive data quality experience from a large and vibrant community with over 11,000 members and hundreds of open source contributors. Data teams all over the world rely on Great Expectations to instrument and understand their data. GX Cloud builds on that trust and community to succeed in our mission.  We are looking for someone to:  Create and uphold a high standard of developer experience within our documentation Work with Product, Engineering, and Developer Relations, to ensure timely, thorough, and accurate documentation of both new and existing product features. Help drive a use-case-based information-architecture approach to organizing content Help us continually improve our authoring and production tools and processes    Specifically, you will:  Write robust technical documentation such as task-based how-to’s, end-to-end tutorials, integration guides, and conceptual guides. Curate clear API references by consulting on the formatting, toolchain, and process. Revise docstrings for clarity based on user feedback. Create functional code examples to include in our documentation, in collaboration with Developer Relations and Engineering. Maintain consistency of style, tone, and voice across a growing body of technical content, including developing style guides for others to do the same Engage in regular review and editing of documentation based on metrics and feedback.    Why join Great Expectations?   Glad you asked! We offer…  A world-class team, with deep roots in open source, cutting-edge software, and data development. We’re backed by some of the best open source and data infrastructure investors in the industry (Index, CRV, and Root Ventures) and are actively cultivating a new cultural blend of excellent data engineering and AI-enabled technical workflows. A fast-growing company with lots of opportunity for learning and personal growth. A front-row seat to the rapid evolution of data science and engineering. Data work is going through a renaissance, and—as the leading provider of a key piece in the new technology ecosystem—GX is right in the middle of it. A kind, curious, and open-minded company culture. We’re always seeking ways to improve ourselves and our processes; and we keep these conversations open to the whole team. We prioritize empowering our team members rather than a command and control hierarchy. A distributed team with lots of flexibility around timing and individual work preferences. We currently have teammates in Arizona, California, Colorado, Georgia, Illinois, Massachusetts, Michigan, Minnesota, Missouri, New Hampshire, New Jersey, New York, North Carolina, Oregon, Tennessee, Texas, Utah, Vermont, Virginia, Washington and Wisconsin. We’d love to add your state. And of course, competitive compensation (base salary + equity package) with available medical, dental, vision insurances, plus a 401(k) with 5% employer match.    The salary range for this role is $146,000-$186,000 plus equity. Our offers are made by taking into account the skill level, experience, and location of the specific candidate. In addition to our compensation package, we offer a competitive suite of benefits.  Job requirements  What you bring:  A solid writing background, including 5+ years as a technical writer for a highly technical SaaS product The ability to clearly and concisely convey technical information to data engineers and data analysts Strong familiarity with \"Docs-as-Code\" approach Strong familiarity with Python and SQL A passion for cultivating a positive developer experience    It’d be nice if you also had:  Previous experience documenting SaaS products with an open-source core component. Direct experience with data work in Python, SQL, Spark, etc. Previous experience working in data engineering, including knowledge of current data engineering tools and trends. Data in your blood. The more curiosity you have about a wide variety of data from all walks of life, the better.    If you don’t think you meet all of the criteria but are still interested in the job, please apply. Nobody checks every box—we’re looking for candidates who are particularly strong in a few areas, and have some interest and capabilities in others.\n"
     ]
    }
   ],
   "source": [
    "# input job description\n",
    "jd_string = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fd11e-da6a-41ba-8ed1-ba8ed4903c8b",
   "metadata": {},
   "source": [
    "### 2) Construct Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b05995-b9a2-4204-9c37-771efbb644a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = lambda resume_string, jd_string : f\"\"\"\n",
    "You are a professional resume optimization expert specializing in tailoring resumes to specific job descriptions. Your goal is to optimize my resume and provide actionable suggestions for improvement to align with the target role.\n",
    "\n",
    "### Guidelines:\n",
    "1. **Relevance**:  \n",
    "   - Prioritize experiences, skills, and achievements **most relevant to the job description**.  \n",
    "   - Remove or de-emphasize irrelevant details to ensure a **concise** and **targeted** resume.\n",
    "   - Limit work experience section to 2-3 most relevant roles\n",
    "   - Limit bullet points under each role to 2-3 most relevant impacts\n",
    "\n",
    "2. **Action-Driven Results**:  \n",
    "   - Use **strong action verbs** and **quantifiable results** (e.g., percentages, revenue, efficiency improvements) to highlight impact.  \n",
    "\n",
    "3. **Keyword Optimization**:  \n",
    "   - Integrate **keywords** and phrases from the job description naturally to optimize for ATS (Applicant Tracking Systems).  \n",
    "\n",
    "4. **Additional Suggestions** *(If Gaps Exist)*:  \n",
    "   - If the resume does not fully align with the job description, suggest:  \n",
    "     1. **Additional technical or soft skills** that I could add to make my profile stronger.  \n",
    "     2. **Certifications or courses** I could pursue to bridge the gap.  \n",
    "     3. **Project ideas or experiences** that would better align with the role.  \n",
    "\n",
    "5. **Formatting**:  \n",
    "   - Output the tailored resume in **clean Markdown format**.  \n",
    "   - Include an **\"Additional Suggestions\"** section at the end with actionable improvement recommendations.  \n",
    "\n",
    "---\n",
    "\n",
    "### Input:\n",
    "- **My resume**:  \n",
    "{resume_string}\n",
    "\n",
    "- **The job description**:  \n",
    "{jd_string}\n",
    "\n",
    "---\n",
    "\n",
    "### Output:  \n",
    "1. **Tailored Resume**:  \n",
    "   - A resume in **Markdown format** that emphasizes relevant experience, skills, and achievements.  \n",
    "   - Incorporates job description **keywords** to optimize for ATS.  \n",
    "   - Uses strong language and is no longer than **one page**.\n",
    "\n",
    "2. **Additional Suggestions** *(if applicable)*:  \n",
    "   - List **skills** that could strengthen alignment with the role.  \n",
    "   - Recommend **certifications or courses** to pursue.  \n",
    "   - Suggest **specific projects or experiences** to develop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "071b2065-775b-4016-b5fd-0b41749c244f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template(resume_string, jd_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654b781-aa60-4a41-a9f1-a578edfad490",
   "metadata": {},
   "source": [
    "### 3) Generate Resume with GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5eb19bc-7cb2-44df-8795-a13695414380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup api client\n",
    "client = OpenAI(api_key=my_sk)\n",
    "\n",
    "# make api call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Expert resume writer\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], \n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "# extract response\n",
    "response_string = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa8d51-860d-42c6-ad60-570d4ac45b23",
   "metadata": {},
   "source": [
    "### 4) Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca46d3f-e22c-4ea0-8425-f62bea16b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate new resume from improvement suggestions\n",
    "response_list = response_string.split(\"## Additional Suggestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b83cb9a-6ac7-4db9-9591-07f48632663c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "# Shaw Talebi  \n",
       "**Email**: [shawhintalebi@gmail.com](mailto:shawhintalebi@gmail.com)  \n",
       "**Homepage**: [shawhintalebi.com](https://shawhintalebi.com/) | **LinkedIn**: [shawhintalebi](https://www.linkedin.com/in/shawhintalebi/)\n",
       "\n",
       "---\n",
       "\n",
       "## Education  \n",
       "**The University of Texas at Dallas**  \n",
       "- PhD, Physics - *May 2022*  \n",
       "- M.S., Physics - *December 2019*  \n",
       "- B.S., Physics - *May 2017*  \n",
       "\n",
       "## Technical Skills  \n",
       "- **Tools**: Python, SQL, GitHub, AWS (SageMaker), Snowflake, Julia  \n",
       "- **Certifications**: AWS Cloud Practitioner Essentials, Data Structure & Algorithms, Tableau  \n",
       "\n",
       "## Relevant Experience  \n",
       "\n",
       "### **Data Science Consultant**  \n",
       "**Shawhin Talebi Ventures LLC** | Plano, Texas *(December 2020 - Present)*  \n",
       "- Implemented full data pipeline to evaluate 300+ biometrics variables, uncovering key insights in human performance.  \n",
       "- Applied unsupervised learning on ICU data to discover sepsis sub-phenotypes, enhancing data integrity and understanding.\n",
       "\n",
       "### **Data Scientist**  \n",
       "**Toyota Financial Services** | Plano, Texas *(June 2022 - July 2023)*  \n",
       "- Redeveloped loan origination models, improving performance by 50% and yielding $2.5 million in value.  \n",
       "- Corrected production credit risk model, impacting over 70% of accounts, and developed monitoring scripts to prevent future issues.\n",
       "\n",
       "### **Research Assistant**  \n",
       "**The University of Texas at Dallas** | Richardson, Texas *(December 2018 - May 2022)*  \n",
       "- Published methodology for EEG band discovery, enhancing characterization precision by 100%.  \n",
       "- Led deployment of Python-based biometrics application providing real-time insights, improving research outcomes.\n",
       "\n",
       "## Publications  \n",
       "1. Talebi S. et al. *Modeling Autonomic Pupillary Responses Using Machine Learning* (2019). [DOI](https://doi.org/10.26717/BJSTR.2019.20.003446).  \n",
       "2. Wijeratne, L.O. et al. *Using Machine Learning for Calibration of Airborne Particulate Sensors*. Sensors 2020.  \n",
       "3. Lary, D.J. et al. *Autonomous Learning with Robotic Teams*. Sensors 2021.  \n",
       "4. Zhang, Y. et al. *Machine Learning for Light Sensor Calibration*. Sensors 2021.  \n",
       "5. Talebi, S. et al. *Data-Driven EEG Band Discovery with Decision Trees*. Sensors 2022.  \n",
       "\n",
       "## Awards and Honors  \n",
       "- **Friends of BrainHealth Visionary New Scientist Award** — Finalist *(September 2021)*  \n",
       "- **Weeks of Welcome Poster Competition** — 3rd Place *(August 2019)*  \n",
       "\n",
       "---\n",
       "\n",
       "**References and additional information available upon request.**\n",
       "\n",
       "---\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a558a-cf64-4baf-8aa3-7d184c5b2b91",
   "metadata": {},
   "source": [
    "#### save new resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e088f6ff-8e49-4ddc-a6bf-f847426a3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as PDF\n",
    "output_pdf_file = \"resumes/resume_new.pdf\"\n",
    "\n",
    "# Convert Markdown to HTML\n",
    "html_content = markdown(response_list[0])\n",
    "\n",
    "# Convert HTML to PDF and save\n",
    "HTML(string=html_content).write_pdf(output_pdf_file, stylesheets=['resumes/style.css'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30adb6b1-68b6-4951-8bfc-19db1ef9eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as markdown\n",
    "output_file = \"resumes/resume_new.md\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(response_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6997a3-47c8-4bf1-9df6-8c1a5ae00858",
   "metadata": {},
   "source": [
    "#### display suggestions for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f7397eb-d6cc-45e5-8cbd-bec6cc0cfc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "1. **Skills to Enhance**:  \n",
       "   - Explore \"Docs-as-Code\" methodologies to align with technical writing requirements.\n",
       "   - Gain experience with data engineering tools like Spark to broaden expertise.\n",
       "\n",
       "2. **Certifications or Courses**:  \n",
       "   - Consider enrolling in courses focused on technical documentation or UX writing.\n",
       "   - Pursue additional training in advanced Python libraries relevant to data engineering.\n",
       "\n",
       "3. **Project Ideas**:  \n",
       "   - Develop a project documenting a hypothetical SaaS product, emphasizing open-source components.\n",
       "   - Create a series of Python tutorials or integration guides to demonstrate technical writing capabilities.\n",
       "```\n",
       "\n",
       "### Explanation of Tailoring:\n",
       "\n",
       "1. **Relevance**: Focused on roles and experiences most aligned with technical writing, data collaboration, and Python expertise as required by the job description.\n",
       "2. **Action-Driven Results**: Emphasized quantifiable impacts and contributions in relevant roles.\n",
       "3. **Keyword Optimization**: Integrated terms like \"data pipeline,\" \"unsupervised learning,\" and \"EEG bands\" to align with the focus on data quality and technical documentation.\n",
       "4. **Formatting**: Structured the resume in a clean Markdown format, ensuring conciseness and clarity.\n",
       "5. **Additional Suggestions**: Provided actionable recommendations to enhance alignment with the target role.\n",
       "\n",
       "This optimized resume showcases your suitability for the Great Expectations role by highlighting your technical experience, relevant skills, and achievements in data science and documentation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d88ce-2f29-4b14-9930-9c331a154f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
